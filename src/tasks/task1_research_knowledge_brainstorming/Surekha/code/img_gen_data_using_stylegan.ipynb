{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Perfomed this experiment on Google Colab Pro version using compute instance."
      ],
      "metadata": {
        "id": "sZWcW5kyi_IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj_WKDGWeg8n",
        "outputId": "5df5ddf9-003b-4e69-8122-b80c754d2609"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  steps followed for doing the setup.\n",
        "  1. Install anaconda or miniconda.\n",
        "  2. Create new environment and activate it.\n",
        "  3. Install python version 3.7 as needed for StyleGAN2-ADA model.\n",
        "  4. Install all the compatible libraries within activate conda environment either using pip or conda.\n",
        "    or specify all the packages within requirements.txt file and run using pip command.\n",
        "\n",
        "Note:- 1. All the below specified commands are done through command line within Google Colab Pro compute instance.\n",
        "       2. For the versions comptability please refer the original github repo i.e https://github.com/NVlabs/stylegan2-ada-pytorch/tree/main\n",
        "'''\n",
        "\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh\n",
        "!conda create -n crop_disease\n",
        "!conda activate crop_disease\n",
        "!conda install python=3.7\n",
        "!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3\n",
        "  # or\n",
        "!pip install -r ./requirements.txt -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "neUAddXy1p8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Credit to Authors Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen,\n",
        "    Timo Aila of Training Generative Adversarial Networks with Limited Data'''\n",
        "\n",
        "# clone the git repo\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An6itCiwerxN",
        "outputId": "e2c1e65d-488f-4eff-f243-7e4c86fcbabe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 7.76 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/stylegan2-ada-pytorch"
      ],
      "metadata": {
        "id": "lr_15DECeuKW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating images for random seeds using pretrained model on rice leaf images.\n",
        "'''\n",
        "  No need to create the out_dir.\n",
        "  Create the pretrained directory and save the pretrained model downloaded from this link\n",
        "  https://drive.google.com/file/d/1K2oPovRjbULI2k6NqqFCWfeXhI0IVdbd/view?usp=sharing\n",
        "'''\n",
        "!python3 generate.py --outdir=out_dir --trunc=1 --seeds=85,265 --network=./pretrained/network-snapshot-005360.pkl"
      ],
      "metadata": {
        "id": "FUifa2060Yfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating list of images for random seeds using pretrained model on rice leaf images.\n",
        "!python3 generate.py --outdir=out_dir --trunc=1 --seeds=100-110 --network=./pretrained/network-snapshot-005360.pkl"
      ],
      "metadata": {
        "id": "b8QmklHU0bVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  After generating the synthetic images use Laplacian filter to remove the blurry ones.\n",
        "  Code for Laplacian filter taken from the github repo https://github.com/yunusa2k2/GANLapRice.\n",
        "\n",
        "  Credit to Authors Yunusa Haruna, Shiyin Qin and Mesmin J. Mbyamm Kiki for the inspiration.\n",
        "'''\n",
        "!python Laplacian.py --images==./out/"
      ],
      "metadata": {
        "id": "2FpyiTB02uog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' No need to run.\n",
        "    Did it to download generated images from google colab drive.\n",
        "'''\n",
        "\n",
        "!zip -r /content/drive/MyDrive/style_gan/generated_images.zip /content/stylegan2-ada-pytorch/out/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMo5vdBW8ZfP",
        "outputId": "2a22cded-af37-4bbb-a4e7-10ecc2dd68e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/stylegan2-ada-pytorch/out/ (stored 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0108.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0105.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0107.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0100.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0085.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0103.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0104.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0109.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0106.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0265.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0110.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0101.png (deflated 0%)\n",
            "  adding: content/stylegan2-ada-pytorch/out/seed0102.png (deflated 0%)\n"
          ]
        }
      ]
    }
  ]
}