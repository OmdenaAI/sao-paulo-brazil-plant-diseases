{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZWcW5kyi_IT"
      },
      "source": [
        "# Perfomed this experiment on Google Colab Pro version using compute instance.\n",
        "GPU: Tesla V100-SXM2-16GB <br>\n",
        "Dataset: Soybean Disease Dataset: https://www.kaggle.com/datasets/shuvoalok98/soybean-disease-dataset <br>\n",
        "Model is trained on images from single category i.e Caterpillar for 1000 kimg \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj_WKDGWeg8n",
        "outputId": "5df5ddf9-003b-4e69-8122-b80c754d2609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neUAddXy1p8c"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  steps followed for doing the setup.\n",
        "  1. Install anaconda or miniconda.\n",
        "  2. Create new environment and activate it.\n",
        "  3. Install python version 3.7 as needed for StyleGAN2-ADA model.\n",
        "  4. Install all the compatible libraries within activate conda environment either using pip or conda.\n",
        "    or specify all the packages within requirements.txt file and run using pip command.\n",
        "\n",
        "Note:- 1. All the below specified commands are done through command line within Google Colab Pro compute instance.\n",
        "       2. For the versions comptability please refer the original github repo i.e https://github.com/NVlabs/stylegan2-ada-pytorch/tree/main\n",
        "'''\n",
        "\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh\n",
        "!conda create -n image_gen\n",
        "!conda activate image_gen\n",
        "!conda install python=3.7\n",
        "!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3\n",
        "  # or Mention all the dependencies in requirements.txt file\n",
        "!pip install -r ./requirements.txt -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Note: You may also need to install packages: psutil and scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An6itCiwerxN",
        "outputId": "e2c1e65d-488f-4eff-f243-7e4c86fcbabe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 7.76 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "''' Credit to Authors Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen,\n",
        "    Timo Aila of Training Generative Adversarial Networks with Limited Data'''\n",
        "\n",
        "# clone the git repo\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lr_15DECeuKW"
      },
      "outputs": [],
      "source": [
        "!cd /content/stylegan2-ada-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "''' Before training/finetuning the model transform the dataset to 256x256 resolution to make it of equal dimensions.\n",
        "    Also here I am trying to lower the dimension to fasten up the training process.\n",
        "    original resoultion is 500 x 500 '''\n",
        "\n",
        "''' Before running this you may need to convert the images to either RGB or Grayscale'''\n",
        "!python convert_modes_for_imgs.py <input_dir> <out_dir>\n",
        "\n",
        "''' After this run the below command, but make sure that the source path is the out_dir from the above command, because\n",
        "    we are using only RGB/grayscale images. '''\n",
        "\n",
        "\n",
        "# Here we are zipping the dataset file as needed in this format for the model.\n",
        "!python dataset_tool.py --source=../drive/MyDrive/raw_data/data/soybean_data/Caterpillar --dest=./datasets/caterpillar.zip --width=256 --height=256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the model on custom dataset\n",
        "\n",
        "!python train.py --outdir=./training-runs --data=./datasets/caterpillar.zip --gpus=1 --kimg=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In case if you want to resume the training from certain checkpoint.\n",
        "# Please update the pkl file name accordingly depending on the checkpoint you want to resume.\n",
        "\n",
        "!python train.py --outdir=./training-runs --data=./datasets/caterpillar.zip --gpus=1 --kimg=1000 â€”-resume=./training-runs/00001-caterpillar-auto1-kimg1000/network-snapshot-000200.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUifa2060Yfm"
      },
      "outputs": [],
      "source": [
        "''' Generating images for random seeds using pretrained model on soybean leaf images.\n",
        "    The model has been trained for 1000 kimg, hence used the last snapshot for generating random seeds.\n",
        "'''\n",
        "\n",
        "!python3 generate.py --outdir=out_dir --trunc=1 --seeds=85,265,297,849 --network=./training-runs/00004-caterpillar-auto1-kimg1000/network-snapshot-001000.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8QmklHU0bVV"
      },
      "outputs": [],
      "source": [
        "# Generating list of images using trained model on soybean leaf images.\n",
        "\n",
        "!python3 generate.py --outdir=out_dir --trunc=1 --seeds=0-2000 --network=./training-runs/00004-caterpillar-auto1-kimg1000/network-snapshot-001000.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FpyiTB02uog"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  After generating the synthetic images use Laplacian filter to remove the blurry ones.\n",
        "  Code for Laplacian filter taken from the github repo https://github.com/yunusa2k2/GANLapRice.\n",
        "\n",
        "  Credit to Authors Yunusa Haruna, Shiyin Qin and Mesmin J. Mbyamm Kiki for the inspiration.\n",
        "'''\n",
        "!python Laplacian.py --images==./out/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
