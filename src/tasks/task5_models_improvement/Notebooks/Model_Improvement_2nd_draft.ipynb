{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0LR6_Qe4wqU"
   },
   "source": [
    "# 1. Load the Data\n",
    "\n",
    "* First connect the drive to the Lab before any processes.\n",
    "* Load all the available data that we have available to our project.\n",
    "* It may impede the process and speed but I think reducing the bactch number will help.\n",
    "* Splitting data into training, validation, and test sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uqT1U1Pt41D8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Cn1s2L_1GkD-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader, Dataset, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FpoCSEA45Ip",
    "outputId": "0e520a14-a360-4bc0-9305-dc23f503592e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER_Cx_r45OmK"
   },
   "outputs": [],
   "source": [
    "root_folders = [\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/01_test_and_val_dataset\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/02_test_and_val_dataset_improved\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/03_test_and_val_dataset_resampled\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/04_test_and_val_dataset_resampled_resized\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/05_train_dataset\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/06_train_deduplicated\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/07_train_deduplicated_dataset\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/08_train_data\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/09_train_data_resized\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/10_train_pal_data\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/11_test_pal_data\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Data/12_val_pal_data\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Define a combined folder to store the merged data\n",
    "combined_folder = \"/content/drive/MyDrive/CoWorkStuff/CombinedData\"\n",
    "\n",
    "# Create the combined dataset folder if it doesn't exist\n",
    "os.makedirs(combined_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCOpFHC85hF_"
   },
   "outputs": [],
   "source": [
    "# Define a dictionary to map subfolder names to class labels\n",
    "class_mapping = {\n",
    "    'subfolder1': 'Cercospora',\n",
    "    'subfolder2': 'Healthy',\n",
    "    'subfolder3': 'Miner',\n",
    "    'subfolder4': 'Phoma',\n",
    "    'subfolder5': 'Rust'\n",
    "    }\n",
    "\n",
    "# Move subfolder content into the combined folder\n",
    "def move_subfolder_content(src, dst):\n",
    "  for item in os.listdir(src):\n",
    "    s = os.path.join(src, item)\n",
    "    d = os.path.join(dst, item)\n",
    "    if os.path.isdir(s):\n",
    "      os.makedirs(d, exist_ok=True)\n",
    "      move_subfolder_content(s, d)\n",
    "    else:\n",
    "      shutil.move(s, d)\n",
    "\n",
    "# Combine data from different root folders into the combined folder\n",
    "for root_folder in root_folders:\n",
    "  for subfolder in os.listdir(root_folder):\n",
    "    subfolder_path = os.path.join(root_folder, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "      # Get the class label from the dictionary\n",
    "      class_label = class_mapping.get(subfolder, 'Other')\n",
    "      # Create a target folder based on the class label\n",
    "      target_folder = os.path.join(combined_folder, class_label)\n",
    "      # Create the target folder if it doesn't exist\n",
    "      os.makedirs(target_folder, exist_ok=True)\n",
    "      # Move the subfolder content to the target folder\n",
    "      move_subfolder_content(subfolder_path, target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sfn9ovJK5maG"
   },
   "outputs": [],
   "source": [
    "# Recursively collect image files from the folder(s)\n",
    "def collect_image_files(folder):\n",
    "  image_files = []\n",
    "  for root, _, files in os.walk(folder):\n",
    "    for file in files:\n",
    "      if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
    "        image_files.append(os.path.join(root, file))\n",
    "  return image_files\n",
    "\n",
    "# Shuffle and split data for each class\n",
    "def shuffle_and_split_data(root_folder, target_root, train_ratio, val_ratio):\n",
    "  for class_label in os.listdir(root_folder):\n",
    "    class_folder = os.path.join(root_folder, class_label)\n",
    "\n",
    "    if os.path.isdir(class_folder):\n",
    "      # List all files in the class folder\n",
    "      class_data = collect_image_files(class_folder)\n",
    "      random.shuffle(class_data)\n",
    "\n",
    "      # Split the data into train, validation, and test sets\n",
    "      num_samples = len(class_data)\n",
    "      num_train = int(train_ratio * num_samples)\n",
    "      num_val = int(val_ratio * num_samples)\n",
    "\n",
    "      train_data = class_data[:num_train]\n",
    "      val_data = class_data[num_train:num_train + num_val]\n",
    "      test_data = class_data[num_train + num_val:]\n",
    "\n",
    "      # Create target folders\n",
    "      train_folder = os.path.join(target_root, 'train', class_label)\n",
    "      val_folder = os.path.join(target_root, 'val', class_label)\n",
    "      test_folder = os.path.join(target_root, 'test', class_label)\n",
    "\n",
    "      os.makedirs(train_folder, exist_ok=True)\n",
    "      os.makedirs(val_folder, exist_ok=True)\n",
    "      os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "      # Move data to respective folders\n",
    "      move_data(train_data, class_folder, train_folder)\n",
    "      move_data(val_data, class_folder, val_folder)\n",
    "      move_data(test_data, class_folder, test_folder)\n",
    "\n",
    "# Define move_data\n",
    "def move_data(data_list, source_folder, dest_folder):\n",
    "  for data in data_list:\n",
    "    src_path = os.path.join(source_folder, data)\n",
    "    dst_path = os.path.join(dest_folder, data)\n",
    "    if os.path.exists(src_path):\n",
    "      shutil.move(src_path, dst_path)\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.65\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.20\n",
    "\n",
    "# Define the target root folder for the train, val, and test sets\n",
    "target_root = \"/content/drive/MyDrive/CoWorkStuff/SplitData\"\n",
    "\n",
    "# Create the train, validation, and test folders\n",
    "os.makedirs(os.path.join(target_root, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(target_root, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(target_root, 'test'), exist_ok=True)\n",
    "\n",
    "# Shuffle and split data for each class\n",
    "shuffle_and_split_data(combined_folder, target_root, train_ratio, val_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrwx1WY45yoe"
   },
   "source": [
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_YUsqOR56gg"
   },
   "source": [
    "# 2. Load the **Models Pre-trained** by our talented model building team\n",
    "\n",
    "* Select as many models that were created.\n",
    "* If possible, check the best performing one or possibly fine tunning them.\n",
    "* Preferrably ensamble the models to make it more solid and stronger.\n",
    "* Without any particular order or favouritism, for the pre-trained models; credit, thanks and props goes out to the following collaborators:\n",
    "  - Darshan\n",
    "  - Lucas\n",
    "  - Dimitra\n",
    "  - Juan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "obKFnhRcH7Oh"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load .h5 model (TensorFlow/Keras)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_01 \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/model_CNN1_BRACOL.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:115\u001b[0m, in \u001b[0;36m_BaseOptimizer._process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated in `optimizer_experimental.Optimizer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, please check the docstring for valid arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m         k,\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid argument, kwargs should be empty \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for `optimizer_experimental.Optimizer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
     ]
    }
   ],
   "source": [
    "# Load .h5 model (TensorFlow/Keras)\n",
    "model_01 = keras.models.load_model(\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Models/model_CNN1_BRACOL.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r3GbPt2l7zHS"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_02 \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/baseline_resnet50.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:115\u001b[0m, in \u001b[0;36m_BaseOptimizer._process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated in `optimizer_experimental.Optimizer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, please check the docstring for valid arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m         k,\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid argument, kwargs should be empty \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for `optimizer_experimental.Optimizer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
     ]
    }
   ],
   "source": [
    "model_02 = keras.models.load_model(\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Models/baseline_resnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tWHdBOp74Cx"
   },
   "outputs": [],
   "source": [
    "model_03 = keras.models.load_model(\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Models/without_cersc_and_healthy_resnet50_deduplicated_mix_val_train_67acc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoakKuqi79An"
   },
   "outputs": [],
   "source": [
    "model_04 = keras.models.load_model(\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Models/without_cersc_and_healthy_resnet50_deduplicated.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5s_awxJ8FOE"
   },
   "outputs": [],
   "source": [
    "model_05 = keras.models.load_model(\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Models/without_cersc_and_healthy_resnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ3Pmbur8I-_"
   },
   "outputs": [],
   "source": [
    "model_06 = keras.models.load_model(\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Models/without_cersc_resnet50_deduplicated_mix_val_train_75acc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmxXf0SR6Vlc",
    "outputId": "174fb246-658c-431c-cfe6-7abd20b92e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CoffeeLeafClassifier(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=115200, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .pth model (PyTorch)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class CoffeeLeafClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CoffeeLeafClassifier, self).__init__()\n",
    "\n",
    "    # Convolutional layers\n",
    "    self.conv_layers = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(32, 64, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "    # Fully connected layers\n",
    "    self.fc_layers = nn.Sequential(\n",
    "        nn.Linear(128 * 30 * 30, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, 5) # 5 classes\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv_layers(x)\n",
    "    x = x.view(x.size(0), -1) # Flatten the output\n",
    "    x = self.fc_layers(x)\n",
    "    return x\n",
    "\n",
    "model_path = os.path.join(\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Models/cnn_strategy1_weighted_loss\",\n",
    "    \"/content/drive/MyDrive/CoWorkStuff/Models/cnn_strategy1_weighted_loss/coffee_leaf_classifier.pth\")\n",
    "model_07 = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model_07.to(device)\n",
    "model_07.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kN8brL36y3y",
    "outputId": "d08514ea-599e-4af9-d5ca-ff04096a7d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "# Load the MobileNetV2 model with random weights\n",
    "model_08 = MobileNetV2(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 107 layers, found 5 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_01 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Replace with the appropriate architecture\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the pre-trained weights\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel_01\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/baseline_resnet50.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\saving\\hdf5_format.py:817\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    815\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[1;32m--> 817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    821\u001b[0m     )\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[0;32m    825\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 107 layers, found 5 saved layers."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model_01 = keras.applications.ResNet50(weights=None, include_top=True)  # Replace with the appropriate architecture\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model_01.load_weights(\"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/baseline_resnet50.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load .h5 model (TensorFlow/Keras)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_01 \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/baseline_resnet50.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:115\u001b[0m, in \u001b[0;36m_BaseOptimizer._process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated in `optimizer_experimental.Optimizer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, please check the docstring for valid arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m         k,\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid argument, kwargs should be empty \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for `optimizer_experimental.Optimizer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
     ]
    }
   ],
   "source": [
    "# Load .h5 model (TensorFlow/Keras)\n",
    "model_01 = keras.models.load_model(\n",
    "    \"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/baseline_resnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .h5 model (TensorFlow/Keras)\n",
    "model_02 = keras.models.load_model(\n",
    "    \"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/model_CNN1_BRACOL.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .h5 model (TensorFlow/Keras)\n",
    "model_03 = keras.models.load_model(\n",
    "    \"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/without_cersc_and_healthy_resnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .h5 model (TensorFlow/Keras)\n",
    "model_04 = keras.models.load_model(\n",
    "    \"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/without_cersc_and_healthy_resnet50_deduplicated.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .h5 model (TensorFlow/Keras)\n",
    "model_05 = keras.models.load_model(\n",
    "    \"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/without_cersc_and_healthy_resnet50_deduplicated_mix_val_train_67acc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load .h5 model (TensorFlow/Keras)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_06 \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/without_cersc_resnet50_deduplicated_mix_val_train_75acc.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:115\u001b[0m, in \u001b[0;36m_BaseOptimizer._process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated in `optimizer_experimental.Optimizer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, please check the docstring for valid arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m         k,\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid argument, kwargs should be empty \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for `optimizer_experimental.Optimizer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
     ]
    }
   ],
   "source": [
    "# Load .h5 model (TensorFlow/Keras)\n",
    "model_06 = keras.models.load_model(\n",
    "    \"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/without_cersc_resnet50_deduplicated_mix_val_train_75acc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CoffeeLeafClassifier(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=115200, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .pth model (PyTorch)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class CoffeeLeafClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CoffeeLeafClassifier, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 30 * 30, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 5) # 5 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the output\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "model_path = os.path.join(\n",
    "    \"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/cnn_strategy1_weighted_loss\",\n",
    "    \"C:/Users/Sabunity Ltd/Desktop/task5_models_improvement/Models/cnn_strategy1_weighted_loss/coffee_leaf_classifier.pth\")\n",
    "model_07 = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model_07.to(device)\n",
    "model_07.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "# Load the MobileNetV2 model with random weights\n",
    "model_08 = MobileNetV2(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZq8yD-mLQq3"
   },
   "source": [
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQNj1qTwLTsx"
   },
   "source": [
    "# 3. Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DhZfROQAE3j"
   },
   "outputs": [],
   "source": [
    "# path to test data folder\n",
    "val_data_01 = \"/content/drive/MyDrive/CoWorkStuff/SplitData/val\"\n",
    "\n",
    "# lists to store images and their labels\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "# list to store bad image paths\n",
    "bad_file_list = []\n",
    "bad_count = 0\n",
    "\n",
    "# list of models\n",
    "models = [\n",
    "    ('model_01', model_01),\n",
    "    ('model_02', model_02),\n",
    "    ('model_03', model_03),\n",
    "    ('model_04', model_04),\n",
    "    ('model_05', model_05),\n",
    "    ('model_06', model_06),\n",
    "    ('model_07', model_07),\n",
    "    ('model_08', model_08)\n",
    "    ]\n",
    "\n",
    "# batch size for loading and preprocessing images\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L2kdXnKAMTh"
   },
   "outputs": [],
   "source": [
    "# Iterate through the subfolders (class labels) the test data folder\n",
    "for class_label in os.listdir(val_data_01):\n",
    "  class_folder = os.path.join(val_data_01, class_label)\n",
    "\n",
    "  # NumPy to efficiently load and preprocess images in batches\n",
    "  image_paths = [os.path.join(class_folder, image_file) for image_file in os.listdir(class_folder)]\n",
    "  num_images = len(image_paths)\n",
    "  num_batches = (num_images + batch_size - 1) // batch_size\n",
    "\n",
    "  for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, num_images)\n",
    "    batch_paths = image_paths[start_idx:end_idx]\n",
    "\n",
    "    try:\n",
    "      images = np.array([img_to_array(load_img(image_path, target_size=(224, 224))) for image_path in batch_paths])\n",
    "      images = images / 255.0\n",
    "      test_images.extend(images)\n",
    "      test_labels.extend([class_label] * len(images))\n",
    "\n",
    "    except Exception as e:\n",
    "      # Handle bad images by appending their paths to the bad_file_list\n",
    "      bad_file_list.extend(batch_paths)\n",
    "      bad_count += len(batch_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnKQKjyiMUoS"
   },
   "outputs": [],
   "source": [
    "# Convert the lists to NumPy arrays\n",
    "X_test = np.array(test_images)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WykUoHO2MUSc"
   },
   "outputs": [],
   "source": [
    "# Create a majority voting classifier\n",
    "voting_classifier = VotingClassifier(estimators=models, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Im6TbuOD-Tg6"
   },
   "outputs": [],
   "source": [
    "voting_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "xN5I5-irQbxf",
    "outputId": "82ff1b35-f6cc-450d-c6ce-82c53a1c05c4"
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-74b660288270>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make predictions using the ensemble model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mensemble_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoting_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoting\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"soft\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mmaj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Make predictions using the ensemble model\n",
    "ensemble_predictions = voting_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YvKXuEzQb-z"
   },
   "outputs": [],
   "source": [
    "# Now you can evaluate the ensemble model's performance\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "\n",
    "print(f'Ensemble Model Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Number of Bad Images: {bad_count}')\n",
    "print('Bad Image Paths:')\n",
    "for bad_path in bad_file_list:\n",
    "    print(bad_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3y8betYrBaye"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs69qObABd5L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
